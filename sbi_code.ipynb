{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b9616f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/stefanradev93/bayesflow.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34660fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model_1:\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from scipy.stats import uniform, entropy, kurtosis, skew\n",
    "from bayesflow.workflows import BasicWorkflow\n",
    "from bayesflow.approximators import ContinuousApproximator\n",
    "from bayesflow.networks import CouplingFlow\n",
    "from bayesflow.adapters import Adapter\n",
    "import bayesflow.diagnostics as bf_diag\n",
    "import bayesflow as bf\n",
    "from scipy.stats import uniform\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41311849",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '/content/drive/My Drive/Synthetic_Eye-Tracking_Data.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e846367",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['fix_duration'] = df['Fixation_End'] - df['Fixation_Start']\n",
    "df['word_idx'] = df['Word_ID']\n",
    "df['word_length'] = df['Word_Length']\n",
    "df['frequency'] = df['Word_Frequency']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c48cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "records = df[['word_idx', 'fix_duration']].to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc3d3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model_1:\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 1. Define Priors\n",
    "priors = {\n",
    "    'nu': uniform(loc=0.1, scale=0.9),\n",
    "    'r': uniform(loc=5.0, scale=15.0),\n",
    "    'mu_T': uniform(loc=150.0, scale=150.0)\n",
    "}\n",
    "\n",
    "def sample_prior():\n",
    "    return {\n",
    "        'nu': priors['nu'].rvs(),\n",
    "        'r': priors['r'].rvs(),\n",
    "        'mu_T': priors['mu_T'].rvs()\n",
    "    }\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 2. SWIFT-inspired Simulation\n",
    "\n",
    "def compute_saliency(activations, eta=0.01):\n",
    "    scaled = activations / eta\n",
    "    exp_vals = np.exp(scaled - np.max(scaled))\n",
    "    return exp_vals / np.sum(exp_vals)\n",
    "\n",
    "def simulate(theta, num_words=30, alpha=9, eta=2.0, max_steps=100):\n",
    "    nu, r, mu_T = theta['nu'], theta['r'], theta['mu_T']\n",
    "    position = 0\n",
    "    activations = np.zeros(num_words)\n",
    "    fixations = []\n",
    "    visited = set()\n",
    "\n",
    "    for _ in range(max_steps):\n",
    "        for i in range(num_words):\n",
    "            dist = abs(i - position)\n",
    "            activations[i] = r * np.exp(-dist**2 / (2 * nu**2))\n",
    "\n",
    "        saliency_probs = compute_saliency(activations, eta)\n",
    "        saliency_probs = saliency_probs / saliency_probs.sum()\n",
    "\n",
    "        next_position = np.random.choice(num_words, p=saliency_probs)\n",
    "\n",
    "        rate = alpha / mu_T\n",
    "        duration = np.random.gamma(shape=alpha, scale=1 / rate)\n",
    "\n",
    "        fixations.append({'word_idx': next_position + 1, 'fix_duration': duration})\n",
    "        visited.add(next_position)\n",
    "\n",
    "        if len(visited) == num_words:\n",
    "            break\n",
    "\n",
    "        position = next_position\n",
    "\n",
    "    return fixations\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 3. Summary Statistics Extractor\n",
    "\n",
    "def extract_summary_stats(fixations):\n",
    "    words = np.array([f['word_idx'] for f in fixations])\n",
    "    durs = np.array([f['fix_duration'] for f in fixations])\n",
    "    saccades = np.diff(words)\n",
    "\n",
    "    return np.array([\n",
    "        durs.mean(),\n",
    "        durs.std(),\n",
    "        np.mean(saccades > 1),\n",
    "        np.mean(saccades < 0),\n",
    "        np.mean(np.abs(saccades)),\n",
    "        skew(durs),\n",
    "        kurtosis(durs),\n",
    "        entropy(np.bincount(words) + 1)\n",
    "    ], dtype=np.float32)\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 4. Generate Training Dataset\n",
    "\n",
    "def simulate_and_extract():\n",
    "    theta = sample_prior()\n",
    "    fixations = simulate(theta)\n",
    "    stats = extract_summary_stats(fixations)\n",
    "    return np.array([theta['nu'], theta['r'], theta['mu_T']], dtype=np.float32), stats\n",
    "\n",
    "N = 10000\n",
    "theta_list, stats_list = [], []\n",
    "\n",
    "for _ in range(N):\n",
    "    theta, stats = simulate_and_extract()\n",
    "    theta_list.append(theta)\n",
    "    stats_list.append(stats)\n",
    "\n",
    "theta_train = np.array(theta_list)\n",
    "x_train = np.array(stats_list)\n",
    "\n",
    "# Optional: validation split\n",
    "theta_val = theta_train[:1000]\n",
    "x_val = x_train[:1000]\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 5. Inference Network\n",
    "\n",
    "inference_network = CouplingFlow(num_params=3)\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 6. BayesFlow Setup\n",
    "\n",
    "class IdentityAdapter:\n",
    "    def adapt(self, sim_out):\n",
    "        return sim_out['parameters'], sim_out['summary_conditions']\n",
    "\n",
    "approximator = ContinuousApproximator(\n",
    "    adapter=IdentityAdapter(),\n",
    "    inference_network=inference_network\n",
    ")\n",
    "\n",
    "workflow = BasicWorkflow(\n",
    "    approximator=approximator,\n",
    "    inference_variables=[\"theta\"],\n",
    "    inference_conditions=[\"x\"]\n",
    ")\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 7. Training\n",
    "\n",
    "workflow.fit_offline(\n",
    "    data={\"x\": x_train, \"theta\": theta_train},\n",
    "    validation_data={\"x\": x_val, \"theta\": theta_val},\n",
    "    epochs=50,\n",
    "    batch_size=64,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 8. Real Observed Fixation Data\n",
    "\n",
    "real_fixations = records\n",
    "obs_summary = extract_summary_stats(real_fixations).reshape(1, -1)\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 9. Posterior Sampling and Prediction\n",
    "\n",
    "posterior_samples = workflow.sample(\n",
    "    conditions={\"x\": obs_summary},\n",
    "    num_samples=1000\n",
    ")\n",
    "\n",
    "samples_array = posterior_samples[\"theta\"].squeeze(0)\n",
    "posterior_mean = samples_array.mean(axis=0)\n",
    "posterior_std = samples_array.std(axis=0)\n",
    "\n",
    " print(\"\\nPosterior Mean Estimates:\")\n",
    " print(f\"ν (nu): {posterior_mean[0]:.4f} ± {posterior_std[0]:.4f}\")\n",
    " print(f\"r:      {posterior_mean[1]:.4f} ± {posterior_std[1]:.4f}\")\n",
    " print(f\"μ_T:    {posterior_mean[2]:.4f} ± {posterior_std[2]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b29022d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model_1:\n",
    "def parameter_recovery(workflow, num_cases=200):\n",
    "    true_params, est_means = [], []\n",
    "\n",
    "    for _ in range(num_cases):\n",
    "        theta_true, x = simulate_and_extract()\n",
    "        posterior = workflow.sample(conditions={\"x\": x.reshape(1, -1)}, num_samples=500)[\"theta\"].squeeze(0)\n",
    "        true_params.append(theta_true)\n",
    "        est_means.append(posterior.mean(axis=0))\n",
    "\n",
    "    true_params = np.array(true_params)\n",
    "    est_means = np.array(est_means)\n",
    "\n",
    "    for i, name in enumerate(['nu', 'r', 'mu_T']):\n",
    "        corr = np.corrcoef(true_params[:, i], est_means[:, i])[0, 1]\n",
    "        rmse = np.sqrt(np.mean((true_params[:, i] - est_means[:, i])**2))\n",
    "        print(f\"{name}: corr = {corr:.3f}, RMSE = {rmse:.3f}\")\n",
    "\n",
    "parameter_recovery(workflow, num_cases=200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac56a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model_1:\n",
    "\n",
    "def posterior_predictive_overlay(workflow, obs_summary, real_fixations, num_samples=100, show_individual_curves=True):\n",
    "    posterior = workflow.sample(conditions={\"x\": obs_summary}, num_samples=num_samples)[\"theta\"].squeeze(0)\n",
    "    simulated_curves = []\n",
    "\n",
    "    for theta_sample in posterior:\n",
    "        fix = simulate({'nu': theta_sample[0], 'r': theta_sample[1], 'mu_T': theta_sample[2]})\n",
    "        fix_durs = [f['fix_duration'] for f in fix]\n",
    "\n",
    "        target_len = len(real_fixations)\n",
    "        if len(fix_durs) >= target_len:\n",
    "            simulated_curves.append(fix_durs[:target_len])\n",
    "        else:\n",
    "            padded = fix_durs + [np.nan] * (target_len - len(fix_durs))\n",
    "            simulated_curves.append(padded)\n",
    "\n",
    "    simulated_curves = np.array(simulated_curves)\n",
    "    if simulated_curves.shape[0] == 0:\n",
    "        print(\"No simulated sequences matched observed length.\")\n",
    "        return\n",
    "\n",
    "    mean_curve = np.nanmean(simulated_curves, axis=0)\n",
    "    observed = [f['fix_duration'] for f in real_fixations]\n",
    "    x = list(range(1, len(observed) + 1))\n",
    "\n",
    "    for i, curve in enumerate(simulated_curves):\n",
    "      if show_individual_curves:\n",
    "        if i == 0:\n",
    "            plt.plot(x, curve, color='blue', alpha=0.1)  # Actual curve\n",
    "            plt.plot([], [], color='blue', label='Posterior predictive samples')  # Legend only\n",
    "        else:\n",
    "            plt.plot(x, curve, color='blue', alpha=0.1)\n",
    "    plt.plot(x, mean_curve, color='orange', linestyle='--', label='Posterior predictive mean')\n",
    "    plt.plot(x, observed, color='black', label='Observed', linewidth=2)\n",
    "\n",
    "    plt.xlabel(\"Fixation Index\")\n",
    "    plt.ylabel(\"Fixation Duration (ms)\")\n",
    "    plt.title(\"Posterior Predictive Overlay\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "posterior_predictive_overlay(workflow, obs_summary, real_fixations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270999c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model_1:\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 7b. ECDF Calibration Plot using Existing Validation Data\n",
    "\n",
    "import bayesflow.diagnostics as bf_diag\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Run inference on validation summaries\n",
    "posterior_val = workflow.sample(\n",
    "    conditions={\"x\": x_val},\n",
    "    num_samples=1000\n",
    ")[\"theta\"]  # shape: (num_samples, num_val, 3)\n",
    "\n",
    "# BayesFlow expects shape: (num_val, num_samples, num_params)\n",
    "samples = {\"parameters\": posterior_val.transpose(1, 0, 2)}\n",
    "validation_data = {\"parameters\": theta_val}\n",
    "\n",
    "# Parameter names for labeling\n",
    "param_names = ['nu', 'r', 'mu_T']\n",
    "\n",
    "# Plot ECDF with 95% confidence band\n",
    "bf_diag.plots.calibration_ecdf(\n",
    "    samples,\n",
    "    validation_data,\n",
    "    variable_names=param_names,\n",
    "    difference=True,\n",
    "    alpha=0.05\n",
    ")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7002e1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model_1:\n",
    "\n",
    "def parameter_recovery_plot(workflow, num_cases=200):\n",
    "    true_params, est_means = [], []\n",
    "\n",
    "    for _ in range(num_cases):\n",
    "        theta_true, x = simulate_and_extract()\n",
    "        posterior = workflow.sample(conditions={\"x\": x.reshape(1, -1)}, num_samples=500)[\"theta\"].squeeze(0)\n",
    "        true_params.append(theta_true)\n",
    "        est_means.append(posterior.mean(axis=0))\n",
    "\n",
    "    true_params = np.array(true_params)\n",
    "    est_means = np.array(est_means)\n",
    "    param_names = ['nu', 'r', 'mu_T']\n",
    "\n",
    "    for i in range(3):\n",
    "        # Plot scatter\n",
    "        plt.figure(figsize=(5, 4))\n",
    "        plt.scatter(true_params[:, i], est_means[:, i], alpha=0.6)\n",
    "        plt.plot(\n",
    "            [true_params[:, i].min(), true_params[:, i].max()],\n",
    "            [true_params[:, i].min(), true_params[:, i].max()],\n",
    "            'r--', label='Perfect Recovery'\n",
    "        )\n",
    "        plt.xlabel(f\"True {param_names[i]}\")\n",
    "        plt.ylabel(f\"Estimated {param_names[i]}\")\n",
    "        plt.title(f\"Parameter Recovery: {param_names[i]}\")\n",
    "\n",
    "        # Compute RMSE and display on plot\n",
    "        errors = est_means[:, i] - true_params[:, i]\n",
    "        rmse = np.sqrt(np.mean(errors ** 2))\n",
    "        corr = np.corrcoef(true_params[:, i], est_means[:, i])[0, 1]\n",
    "        plt.text(0.05, 0.95, f\"RMSE = {rmse:.4f}\\nCorr = {corr:.4f}\",\n",
    "                 transform=plt.gca().transAxes, fontsize=10,\n",
    "                 verticalalignment='top', bbox=dict(boxstyle=\"round\", facecolor=\"white\", alpha=0.8))\n",
    "\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        print(f\"{param_names[i]}: RMSE = {rmse:.4f}, Correlation = {corr:.4f}\")\n",
    "\n",
    "parameter_recovery_plot(workflow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c30f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model_2:\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 1. Define Priors\n",
    "priors = {\n",
    "    'nu': uniform(loc=0.1, scale=0.9),\n",
    "    'r': uniform(loc=5.0, scale=15.0),\n",
    "    'mu_T': uniform(loc=150.0, scale=150.0)\n",
    "}\n",
    "\n",
    "def sample_prior():\n",
    "    return {\n",
    "        'nu': priors['nu'].rvs(),\n",
    "        'r': priors['r'].rvs(),\n",
    "        'mu_T': priors['mu_T'].rvs()\n",
    "    }\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 2. SWIFT-inspired Simulation\n",
    "\n",
    "def compute_saliency(activations, eta=0.01):\n",
    "    scaled = activations / eta\n",
    "    exp_vals = np.exp(scaled - np.max(scaled))\n",
    "    return exp_vals / np.sum(exp_vals)\n",
    "\n",
    "def simulate(theta, num_words=30, alpha=9, eta=2.0, max_steps=100):\n",
    "    nu, r, mu_T = theta['nu'], theta['r'], theta['mu_T']\n",
    "    position = 0\n",
    "    activations = np.zeros(num_words)\n",
    "    fixations = []\n",
    "    visited = set()\n",
    "\n",
    "    for _ in range(max_steps):\n",
    "        for i in range(num_words):\n",
    "            dist = abs(i - position)\n",
    "            activations[i] = r * np.exp(-dist**2 / (2 * nu**2))\n",
    "\n",
    "        saliency_probs = compute_saliency(activations, eta)\n",
    "        saliency_probs = saliency_probs / saliency_probs.sum()\n",
    "        next_position = np.random.choice(num_words, p=saliency_probs)\n",
    "\n",
    "        rate = alpha / mu_T\n",
    "        duration = np.random.gamma(shape=alpha, scale=1 / rate)\n",
    "\n",
    "        fixations.append({'word_idx': next_position + 1, 'fix_duration': duration})\n",
    "        visited.add(next_position)\n",
    "\n",
    "        if len(visited) == num_words:\n",
    "            break\n",
    "\n",
    "        position = next_position\n",
    "\n",
    "    return fixations\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 3. Learned Summary Network\n",
    "\n",
    "class SummaryNetwork(bf.networks.SummaryNetwork):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.network = keras.Sequential(\n",
    "            [\n",
    "                keras.layers.Dense(400, activation=\"relu\"),\n",
    "                keras.layers.Dense(200, activation=\"relu\"),\n",
    "                keras.layers.Dense(100, activation=\"relu\"),\n",
    "                keras.layers.Dense(50, activation=\"relu\"),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def call(self, x, **kwargs):\n",
    "        summary = self.network(x, training=kwargs.get(\"stage\") == \"training\")\n",
    "        return summary\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 4. Generate Training Dataset\n",
    "\n",
    "MAX_FIXATIONS = 100\n",
    "FEATURES_PER_FIXATION = 2\n",
    "PADDED_LENGTH = MAX_FIXATIONS * FEATURES_PER_FIXATION\n",
    "\n",
    "def pad_fixations(arr):\n",
    "    if len(arr) >= PADDED_LENGTH:\n",
    "        return arr[:PADDED_LENGTH]\n",
    "    padded = np.zeros(PADDED_LENGTH, dtype=np.float32)\n",
    "    padded[:len(arr)] = arr\n",
    "    return padded\n",
    "\n",
    "def simulate_and_extract():\n",
    "    theta = sample_prior()\n",
    "    fixations = simulate(theta)\n",
    "    flat_fix = []\n",
    "    for f in fixations:\n",
    "        flat_fix.extend([f['word_idx'], f['fix_duration']])\n",
    "    return np.array([theta['nu'], theta['r'], theta['mu_T']], dtype=np.float32), pad_fixations(np.array(flat_fix, dtype=np.float32))\n",
    "\n",
    "N = 10000\n",
    "theta_list, fix_list = [], []\n",
    "\n",
    "for _ in range(N):\n",
    "    theta, fix = simulate_and_extract()\n",
    "    theta_list.append(theta)\n",
    "    fix_list.append(fix)\n",
    "\n",
    "theta_train = np.array(theta_list)\n",
    "x_train = np.array(fix_list)\n",
    "\n",
    "theta_val = theta_train[:1000]\n",
    "x_val = x_train[:1000]\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 5. Adapter\n",
    "\n",
    "class FixationAdapter:\n",
    "    def adapt(self, sim_out):\n",
    "        return sim_out[\"parameters\"], sim_out[\"sim_data\"]\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 6. Setup BayesFlow Workflow\n",
    "\n",
    "summary_network = SummaryNetwork()\n",
    "inference_network = CouplingFlow(num_params=3)\n",
    "\n",
    "approximator = ContinuousApproximator(\n",
    "    adapter=FixationAdapter(),\n",
    "    summary_network=summary_network,\n",
    "    inference_network=inference_network\n",
    ")\n",
    "\n",
    "workflow = BasicWorkflow(\n",
    "    approximator=approximator,\n",
    "    inference_variables=[\"theta\"],\n",
    "    inference_conditions=[\"x\"]\n",
    ")\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 7. Training\n",
    "\n",
    "workflow.fit_offline(\n",
    "    data={\"x\": x_train, \"theta\": theta_train},\n",
    "    validation_data={\"x\": x_val, \"theta\": theta_val},\n",
    "    epochs=50,\n",
    "    batch_size=64,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 8. Real Observed Fixation Data\n",
    "\n",
    "real_fixations = records\n",
    "\n",
    "flat_obs = []\n",
    "for f in real_fixations:\n",
    "    flat_obs.extend([f['word_idx'], f['fix_duration']])\n",
    "flat_obs = pad_fixations(np.array(flat_obs, dtype=np.float32)).reshape(1, -1)\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 9. Posterior Sampling and Prediction\n",
    "\n",
    "posterior_samples = workflow.sample(\n",
    "    conditions={\"x\": flat_obs},\n",
    "    num_samples=1000\n",
    ")\n",
    "\n",
    "samples_array = posterior_samples[\"theta\"].squeeze(0)\n",
    "posterior_mean = samples_array.mean(axis=0)\n",
    "posterior_std = samples_array.std(axis=0)\n",
    "\n",
    "print(\"\\nPosterior Mean Estimates:\")\n",
    "print(f\"ν (nu): {posterior_mean[0]:.4f} ± {posterior_std[0]:.4f}\")\n",
    "print(f\"r:      {posterior_mean[1]:.4f} ± {posterior_std[1]:.4f}\")\n",
    "print(f\"μ_T:    {posterior_mean[2]:.4f} ± {posterior_std[2]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29314472",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model_2:\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 10. Posterior Predictive Check Plot\n",
    "\n",
    "\n",
    "def posterior_predictive_overlay(workflow, obs_fixations, real_fixations, num_samples=100, show_individual_curves=True):\n",
    "    # Handle padding and reshaping internally\n",
    "    if isinstance(obs_fixations, list) and isinstance(obs_fixations[0], dict):\n",
    "        flat_obs = []\n",
    "        for f in obs_fixations:\n",
    "            flat_obs.extend([f['word_idx'], f['fix_duration']])\n",
    "        obs_summary = pad_fixations(np.array(flat_obs, dtype=np.float32)).reshape(1, -1)\n",
    "    elif isinstance(obs_fixations, np.ndarray):\n",
    "        obs_summary = obs_fixations.reshape(1, -1)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported format for obs_fixations.\")\n",
    "\n",
    "    posterior = workflow.sample(conditions={\"x\": obs_summary}, num_samples=num_samples)[\"theta\"].squeeze(0)\n",
    "    simulated_curves = []\n",
    "\n",
    "    for theta_sample in posterior:\n",
    "        fix = simulate({'nu': theta_sample[0], 'r': theta_sample[1], 'mu_T': theta_sample[2]})\n",
    "        fix_durs = [f['fix_duration'] for f in fix]\n",
    "\n",
    "        target_len = len(real_fixations)\n",
    "        if len(fix_durs) >= target_len:\n",
    "            simulated_curves.append(fix_durs[:target_len])\n",
    "        else:\n",
    "            padded = fix_durs + [np.nan] * (target_len - len(fix_durs))\n",
    "            simulated_curves.append(padded)\n",
    "\n",
    "    simulated_curves = np.array(simulated_curves)\n",
    "    if simulated_curves.shape[0] == 0:\n",
    "        print(\"No simulated sequences matched observed length.\")\n",
    "        return\n",
    "\n",
    "    mean_curve = np.nanmean(simulated_curves, axis=0)\n",
    "    observed = [f['fix_duration'] for f in real_fixations]\n",
    "    x = list(range(1, len(observed) + 1))\n",
    "\n",
    "    for i, curve in enumerate(simulated_curves):\n",
    "        if show_individual_curves:\n",
    "            if i == 0:\n",
    "                plt.plot(x, curve, color='blue', alpha=0.1)\n",
    "                plt.plot([], [], color='blue', label='Posterior predictive samples')\n",
    "            else:\n",
    "                plt.plot(x, curve, color='blue', alpha=0.1)\n",
    "\n",
    "    plt.plot(x, mean_curve, color='orange', linestyle='--', label='Posterior predictive mean')\n",
    "    plt.plot(x, observed, color='black', label='Observed', linewidth=2)\n",
    "\n",
    "    plt.xlabel(\"Fixation Index\")\n",
    "    plt.ylabel(\"Fixation Duration (ms)\")\n",
    "    plt.title(\"Posterior Predictive Overlay\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Run the overlay using real fixation data (automatic padding)\n",
    "posterior_predictive_overlay(workflow, real_fixations, real_fixations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba1eea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model_2:\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "import bayesflow.diagnostics as bf_diag\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Run inference on validation data\n",
    "posterior_val = workflow.sample(\n",
    "    conditions={\"x\": x_val},\n",
    "    num_samples=1000\n",
    ")[\"theta\"]  # shape: (num_samples, num_val, 3)\n",
    "\n",
    "# Transpose posterior_val to (num_val, num_samples, num_parameters)\n",
    "posterior_val_transposed = posterior_val.transpose(1, 0, 2)\n",
    "\n",
    "# Use theta_val directly for targets\n",
    "estimates = posterior_val_transposed\n",
    "targets = theta_val\n",
    "\n",
    "# Plot ECDF\n",
    "bf_diag.plots.calibration_ecdf(\n",
    "    estimates=estimates,\n",
    "    targets=targets,\n",
    "    variable_names=param_names,\n",
    "    difference=True,\n",
    "    alpha=0.05\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b65eed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model_2:\n",
    "from tqdm import tqdm  # progress bar\n",
    "\n",
    "def parameter_recovery_plot(workflow, num_cases=200):\n",
    "    true_params, est_means = [], []\n",
    "\n",
    "    for _ in tqdm(range(num_cases), desc=\"Simulating + inferring\"):\n",
    "        theta_true, x = simulate_and_extract()\n",
    "        posterior = workflow.sample(\n",
    "            conditions={\"x\": x.reshape(1, -1)},  # x must be shape (1, 200)\n",
    "            num_samples=500\n",
    "        )[\"theta\"].squeeze(0)\n",
    "        true_params.append(theta_true)\n",
    "        est_means.append(posterior.mean(axis=0))\n",
    "\n",
    "    true_params = np.array(true_params)\n",
    "    est_means = np.array(est_means)\n",
    "    param_names = ['nu', 'r', 'mu_T']\n",
    "\n",
    "    for i in range(3):\n",
    "        plt.figure(figsize=(5, 4))\n",
    "        plt.scatter(true_params[:, i], est_means[:, i], alpha=0.6)\n",
    "        plt.plot(\n",
    "            [true_params[:, i].min(), true_params[:, i].max()],\n",
    "            [true_params[:, i].min(), true_params[:, i].max()],\n",
    "            'r--', label='Perfect Recovery'\n",
    "        )\n",
    "        plt.xlabel(f\"True {param_names[i]}\")\n",
    "        plt.ylabel(f\"Estimated {param_names[i]}\")\n",
    "        plt.title(f\"Parameter Recovery: {param_names[i]}\")\n",
    "\n",
    "        # RMSE and Correlation\n",
    "        errors = est_means[:, i] - true_params[:, i]\n",
    "        rmse = np.sqrt(np.mean(errors ** 2))\n",
    "        corr = np.corrcoef(true_params[:, i], est_means[:, i])[0, 1]\n",
    "        plt.text(0.05, 0.95, f\"RMSE = {rmse:.4f}\\nCorr = {corr:.4f}\",\n",
    "                 transform=plt.gca().transAxes, fontsize=10,\n",
    "                 verticalalignment='top', bbox=dict(boxstyle=\"round\", facecolor=\"white\", alpha=0.8))\n",
    "\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        print(f\"{param_names[i]}: RMSE = {rmse:.4f}, Correlation = {corr:.4f}\")\n",
    "\n",
    "# Run the parameter recovery\n",
    "parameter_recovery_plot(workflow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b55a3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison_Model:\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 0. Setup\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 1. Priors\n",
    "\n",
    "priors = {\n",
    "    'nu': uniform(loc=0.1, scale=0.9),\n",
    "    'r': uniform(loc=5.0, scale=15.0),\n",
    "    'mu_T': uniform(loc=150.0, scale=150.0)\n",
    "}\n",
    "\n",
    "def sample_prior():\n",
    "    return {\n",
    "        'nu': priors['nu'].rvs(),\n",
    "        'r': priors['r'].rvs(),\n",
    "        'mu_T': priors['mu_T'].rvs()\n",
    "    }\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 2. Simulation Variants\n",
    "\n",
    "def simulate(theta, num_words=30, alpha=9, eta=1e-3, max_steps=100):\n",
    "    nu, r, mu_T = theta['nu'], theta['r'], theta['mu_T']\n",
    "    position = 0\n",
    "    activations = np.zeros(num_words)\n",
    "    fixations = []\n",
    "    for _ in range(max_steps):\n",
    "        for i in range(num_words):\n",
    "            d = abs(i - position)\n",
    "            activations[i] += r * np.exp(-d**2 / (2 * nu**2))\n",
    "        norm = activations / activations.max() if activations.max() > 0 else activations\n",
    "        sal = np.sin(np.pi * norm)**2\n",
    "        sal = np.where(norm < 1, sal, 0) + eta\n",
    "        probs = sal / sal.sum()\n",
    "        position = np.random.choice(num_words, p=probs)\n",
    "        duration = np.random.gamma(shape=alpha, scale=mu_T / alpha)\n",
    "        fixations.append({'word': position + 1, 'duration': duration})\n",
    "    return fixations\n",
    "\n",
    "def simulate_no_saliency(theta, num_words=10, alpha=9, max_steps=50):\n",
    "    mu_T = theta['mu_T']\n",
    "    fixations = [\n",
    "        {'word': np.random.randint(1, num_words + 1),\n",
    "         'duration': np.random.gamma(alpha, mu_T / alpha)}\n",
    "        for _ in range(max_steps)\n",
    "    ]\n",
    "    return fixations\n",
    "\n",
    "def simulate_random(theta, num_words=10, alpha=9, max_steps=50):\n",
    "    return simulate_no_saliency(theta, num_words, alpha, max_steps)\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 3. Data Processing\n",
    "\n",
    "MAX_FIXATIONS = 100\n",
    "FEATURES_PER_FIX = 2\n",
    "PADDED_LEN = MAX_FIXATIONS * FEATURES_PER_FIX\n",
    "\n",
    "def flatten_fixations(fix):\n",
    "    flat = []\n",
    "    for f in fix:\n",
    "        flat.extend([f['word'], f['duration']])\n",
    "    return np.array(flat, dtype=np.float32)\n",
    "\n",
    "def pad_fixations(fix_array):\n",
    "    if len(fix_array) >= PADDED_LEN:\n",
    "        return fix_array[:PADDED_LEN]\n",
    "    padded = np.zeros(PADDED_LEN, dtype=np.float32)\n",
    "    padded[:len(fix_array)] = fix_array\n",
    "    return padded\n",
    "\n",
    "def simulate_and_prepare(sim_func):\n",
    "    theta = sample_prior()\n",
    "    fix = sim_func(theta)\n",
    "    x = pad_fixations(flatten_fixations(fix))\n",
    "    y = np.array([theta['nu'], theta['r'], theta['mu_T']], dtype=np.float32)\n",
    "    return x, y\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 4. Summary Network\n",
    "\n",
    "class SummaryNetwork(bf.networks.SummaryNetwork):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.network = keras.Sequential([\n",
    "            keras.layers.Input(shape=(PADDED_LEN,)),\n",
    "            keras.layers.Dense(400, activation=\"relu\"),\n",
    "            keras.layers.Dense(200, activation=\"relu\"),\n",
    "            keras.layers.Dense(100, activation=\"relu\"),\n",
    "            keras.layers.Dense(50, activation=\"relu\"),\n",
    "        ])\n",
    "    def call(self, x, **kwargs):\n",
    "        return self.network(x, training=kwargs.get(\"stage\") == \"training\")\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 5. Adapter\n",
    "\n",
    "class IdentityAdapter:\n",
    "    def adapt(self, sim_out):\n",
    "        return sim_out[\"parameters\"], sim_out[\"sim_data\"]\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 6. Training Helper\n",
    "\n",
    "def train_bayesflow_model(sim_func, epochs=50, N=10000):\n",
    "    x_list, theta_list = [], []\n",
    "    for _ in range(N):\n",
    "        x, y = simulate_and_prepare(sim_func)\n",
    "        x_list.append(x)\n",
    "        theta_list.append(y)\n",
    "    x_train = np.array(x_list)\n",
    "    theta_train = np.array(theta_list)\n",
    "    x_val = x_train[:1000]\n",
    "    theta_val = theta_train[:1000]\n",
    "\n",
    "    summary_net = SummaryNetwork()\n",
    "    inference_net = CouplingFlow(num_params=3)\n",
    "\n",
    "    approximator = ContinuousApproximator(\n",
    "        adapter=IdentityAdapter(),\n",
    "        summary_network=summary_net,\n",
    "        inference_network=inference_net\n",
    "    )\n",
    "\n",
    "    workflow = BasicWorkflow(\n",
    "        approximator=approximator,\n",
    "        inference_variables=[\"theta\"],\n",
    "        inference_conditions=[\"x\"]\n",
    "    )\n",
    "\n",
    "    workflow.fit_offline(\n",
    "        data={\"x\": x_train, \"theta\": theta_train},\n",
    "        validation_data={\"x\": x_val, \"theta\": theta_val},\n",
    "        epochs=epochs,\n",
    "        batch_size=64,\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    return workflow\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 7. Train 3 Models\n",
    "\n",
    "print(\"Training full model...\")\n",
    "workflow_full = train_bayesflow_model(simulate)\n",
    "\n",
    "print(\"Training no saliency model...\")\n",
    "workflow_no_sal = train_bayesflow_model(simulate_no_saliency)\n",
    "\n",
    "print(\"Training random model...\")\n",
    "workflow_rand = train_bayesflow_model(simulate_random)\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 8. Evaluate on Ground Truth\n",
    "\n",
    "true_params = {'nu': 0.5, 'r': 10.0, 'mu_T': 200.0}\n",
    "obs_vec = pad_fixations(flatten_fixations(simulate(true_params))).reshape(1, -1)\n",
    "true_vals = np.array([true_params['nu'], true_params['r'], true_params['mu_T']])\n",
    "\n",
    "def get_abs_error(workflow):\n",
    "    posterior = workflow.sample(conditions={\"x\": obs_vec}, num_samples=1000)[\"theta\"].squeeze(0)\n",
    "    mean = posterior.mean(axis=0)\n",
    "    return np.abs(mean - true_vals)\n",
    "\n",
    "err_full = get_abs_error(workflow_full)\n",
    "err_no_sal = get_abs_error(workflow_no_sal)\n",
    "err_rand = get_abs_error(workflow_rand)\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 9. Visualization: Separate Bar Plots for Each Parameter (with value labels)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "labels = ['Full', 'No Saliency', 'Random']\n",
    "errors = {\n",
    "    'ν (nu)': [err_full[0], err_no_sal[0], err_rand[0]],\n",
    "    'r':      [err_full[1], err_no_sal[1], err_rand[1]],\n",
    "    'μ_T':    [err_full[2], err_no_sal[2], err_rand[2]]\n",
    "}\n",
    "\n",
    "for param, values in errors.items():\n",
    "    plt.figure(figsize=(5, 4))\n",
    "    bars = plt.bar(labels, values, color=['C0', 'C1', 'C2'])\n",
    "\n",
    "    # Add value labels on top of each bar\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(\n",
    "            bar.get_x() + bar.get_width() / 2,\n",
    "            height + 0.01 * max(values),  # small offset above bar\n",
    "            f\"{height:.3f}\",\n",
    "            ha='center',\n",
    "            va='bottom',\n",
    "            fontsize=10\n",
    "        )\n",
    "\n",
    "    plt.ylabel(\"Absolute Error\")\n",
    "    plt.title(f\"Parameter Recovery Error: {param}\")\n",
    "    plt.ylim(0, max(values) * 1.2)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.6)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad38d2ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
